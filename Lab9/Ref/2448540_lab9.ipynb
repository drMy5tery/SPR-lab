{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm Based Phoneme Decoding for Speech Recognition\n",
    "\n",
    "## Aim\n",
    "To implement the Viterbi Algorithm in a Hidden Markov Model (HMM) and decode the most probable phoneme sequence for the word \"hello\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "Speech recognition can be modeled as a sequence problem where we want to find the most likely sequence of hidden states (phonemes) given a sequence of observations (acoustic features).\n",
    "\n",
    "In this experiment:\n",
    "- **Hidden States (S)**: Represent phonemes. We have 4 states: $S_1=/h/, S_2=/e/, S_3=/l/, S_4=/o/$.\n",
    "- **Observations (O)**: Represent extracted acoustic feature vectors: $O_1, O_2, O_3, O_4$.\n",
    "- **Goal**: Find the state sequence $Q = q_1, q_2, q_3, q_4$ that maximizes $P(Q|O, \\lambda)$, where $\\lambda$ is the HMM model parameters $(A, B, \\pi)$.\n",
    "\n",
    "The **Viterbi Algorithm** is a dynamic programming algorithm used to find this most likely sequence of hidden states, often called the \"Viterbi path\". It avoids the exponential complexity of checking all possible paths by keeping track of the maximum probability path reaching each state at each time step.\n",
    "\n",
    "### HMM Diagram\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    h((/h/)) -->|1.0| e((/e/))\n",
    "    e -->|1.0| l((/l/))\n",
    "    l -->|1.0| o((/o/))\n",
    "    o -->|1.0| o\n",
    "    \n",
    "    h -.-> O1[O1]\n",
    "    e -.-> O2[O2]\n",
    "    l -.-> O3[O3]\n",
    "    o -.-> O4[O4]\n",
    "```\n",
    "*Note: Transition probabilities shown are simplified for the 'hello' example.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define the HMM Components\n",
    "\n",
    "# States (Phonemes)\n",
    "states = ['h', 'e', 'l', 'o']\n",
    "\n",
    "# Observations (Acoustic Features)\n",
    "observations = ['O1', 'O2', 'O3', 'O4']\n",
    "\n",
    "# Transition Probability Matrix A\n",
    "# A[i][j] = P(state_j | state_i)\n",
    "A = {\n",
    "    'h': {'h': 0.0, 'e': 1.0, 'l': 0.0, 'o': 0.0},\n",
    "    'e': {'h': 0.0, 'e': 0.0, 'l': 1.0, 'o': 0.0},\n",
    "    'l': {'h': 0.0, 'e': 0.0, 'l': 0.0, 'o': 1.0},\n",
    "    'o': {'h': 0.0, 'e': 0.0, 'l': 0.0, 'o': 1.0}\n",
    "}\n",
    "\n",
    "# Emission Probability Matrix B\n",
    "# B[i][k] = P(observation_k | state_i)\n",
    "B = {\n",
    "    'h': {'O1': 1.0, 'O2': 0.0, 'O3': 0.0, 'O4': 0.0},\n",
    "    'e': {'O1': 0.0, 'O2': 1.0, 'O3': 0.0, 'O4': 0.0},\n",
    "    'l': {'O1': 0.0, 'O2': 0.0, 'O3': 1.0, 'O4': 0.0},\n",
    "    'o': {'O1': 0.0, 'O2': 0.0, 'O3': 0.0, 'O4': 1.0}\n",
    "}\n",
    "\n",
    "# Initial Probabilities pi\n",
    "pi = {'h': 1.0, 'e': 0.0, 'l': 0.0, 'o': 0.0}\n",
    "\n",
    "# Observation Sequence to Decode\n",
    "obs_sequence = ['O1', 'O2', 'O3', 'O4']\n",
    "\n",
    "print(\"HMM Components Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explain the Viterbi Algorithm\n",
    "\n",
    "The Viterbi algorithm finds the most likely sequence of hidden states $Q$ given the observations $O$.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1.  **Initialization**: \n",
    "    - Calculate the probability of starting in each state $i$ and generating the first observation $O_1$.\n",
    "    - $\\delta_1(i) = \\pi_i \\cdot B_i(O_1)$\n",
    "    - $\\psi_1(i) = 0$ (Backpointer)\n",
    "\n",
    "2.  **Recursion** (for $t=2$ to $T$):\n",
    "    - For each state $j$, find the best previous state $i$ that maximizes the probability of transitioning to $j$ and emitting $O_t$.\n",
    "    - $\\delta_t(j) = \\max_{i} [\\delta_{t-1}(i) \\cdot A_{ij}] \\cdot B_j(O_t)$\n",
    "    - $\\psi_t(j) = \\arg\\max_{i} [\\delta_{t-1}(i) \\cdot A_{ij}]$\n",
    "\n",
    "3.  **Termination**:\n",
    "    - Find the final best path probability and the last state.\n",
    "    - $P^* = \\max_{i} [\\delta_T(i)]$\n",
    "    - $q_T^* = \\arg\\max_{i} [\\delta_T(i)]$\n",
    "\n",
    "4.  **Backtracking**:\n",
    "    - Trace back from the last state to the first to reconstruct the path.\n",
    "    - $q_t^* = \\psi_{t+1}(q_{t+1}^*)$\n",
    "\n",
    "### Why Log Probabilities?\n",
    "Probabilities in HMMs can become very small (vanishing gradients problem) as we multiply them over long sequences. Using **log probabilities** turns multiplication into addition ($log(a \\cdot b) = log(a) + log(b)$), which is numerically stable.\n",
    "\n",
    "### Flowchart\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    Start([Start]) --> Init[Initialization: t=1]\n",
    "    Init --> Recurse{t <= T?}\n",
    "    Recurse -- Yes --> Calc[Calculate max prob for each state]\n",
    "    Calc --> Store[Store backpointers]\n",
    "    Store --> Inc[t = t + 1]\n",
    "    Inc --> Recurse\n",
    "    Recurse -- No --> Term[Termination: Find max prob at T]\n",
    "    Term --> Back[Backtrack to find path]\n",
    "    Back --> End([End])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Implement the Viterbi Algorithm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def viterbi(states, observations, A, B, pi, obs_seq):\n",
    "    T = len(obs_seq)\n",
    "    N = len(states)\n",
    "    \n",
    "    # Initialize DP table (delta) and backpointer table (psi)\n",
    "    # Rows: States, Cols: Time steps\n",
    "    delta = pd.DataFrame(0.0, index=states, columns=range(T))\n",
    "    psi = pd.DataFrame(\"\", index=states, columns=range(T))\n",
    "    \n",
    "    # 1. Initialization\n",
    "    first_obs = obs_seq[0]\n",
    "    for s in states:\n",
    "        delta.loc[s, 0] = pi[s] * B[s][first_obs]\n",
    "        psi.loc[s, 0] = \"Start\"\n",
    "        \n",
    "    # 2. Recursion\n",
    "    for t in range(1, T):\n",
    "        obs = obs_seq[t]\n",
    "        for s_curr in states:\n",
    "            # Calculate prob of arriving at s_curr from each s_prev\n",
    "            probs = {}\n",
    "            for s_prev in states:\n",
    "                # P(prev) * Transition(prev->curr) * Emission(curr emits obs)\n",
    "                p = delta.loc[s_prev, t-1] * A[s_prev][s_curr] * B[s_curr][obs]\n",
    "                probs[s_prev] = p\n",
    "            \n",
    "            # Find max prob and the state that gave it\n",
    "            max_prob = max(probs.values())\n",
    "            best_prev_state = max(probs, key=probs.get)\n",
    "            \n",
    "            delta.loc[s_curr, t] = max_prob\n",
    "            psi.loc[s_curr, t] = best_prev_state\n",
    "            \n",
    "    # 3. Termination\n",
    "    # Find the state with the highest probability at the last time step\n",
    "    last_col = delta.iloc[:, T-1]\n",
    "    best_path_prob = last_col.max()\n",
    "    last_state = last_col.idxmax()\n",
    "    \n",
    "    # 4. Backtracking\n",
    "    best_path = [\"\"] * T\n",
    "    best_path[T-1] = last_state\n",
    "    \n",
    "    for t in range(T-2, -1, -1):\n",
    "        best_path[t] = psi.loc[best_path[t+1], t+1]\n",
    "        \n",
    "    return best_path, best_path_prob, delta\n",
    "\n",
    "print(\"Viterbi Function Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Run the Algorithm and Show Output\n",
    "\n",
    "best_sequence, probability, dp_table = viterbi(states, observations, A, B, pi, obs_sequence)\n",
    "\n",
    "print(\"Dynamic Programming Table (Delta):\")\n",
    "print(dp_table)\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"Most likely sequence: {' -> '.join(best_sequence)}\")\n",
    "print(f\"Probability: {probability}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Plot Probability Evolution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(dp_table, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "plt.title(\"Viterbi Path Probability Evolution (Heatmap)\")\n",
    "plt.xlabel(\"Time Step (Observations)\")\n",
    "plt.ylabel(\"States (Phonemes)\")\n",
    "plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5], labels=obs_sequence)\n",
    "plt.show()\n",
    "\n",
    "# Line plot visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "for state in states:\n",
    "    plt.plot(range(len(obs_sequence)), dp_table.loc[state], marker='o', label=f'State {state}')\n",
    "\n",
    "plt.title(\"State Probability Progression\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xticks(range(len(obs_sequence)), obs_sequence)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Inference\n",
    "\n",
    "### Conclusion\n",
    "The Viterbi algorithm successfully decoded the observation sequence `['O1', 'O2', 'O3', 'O4']` into the phoneme sequence `h -> e -> l -> o` with a probability of **1.0**.\n",
    "\n",
    "### Why this path?\n",
    "1.  **Initialization**: The initial probability vector $\\pi$ strongly favored starting at 'h' (prob 1.0).\n",
    "2.  **Transitions**: The transition matrix $A$ was set up deterministically for this example (e.g., 'h' only goes to 'e').\n",
    "3.  **Emissions**: The emission matrix $B$ perfectly aligned observations with states (e.g., 'h' emits 'O1' with prob 1.0).\n",
    "\n",
    "In a real-world scenario, these probabilities would not be 0 or 1, but learned from data. The Viterbi algorithm is robust because it considers the *entire* sequence context to resolve ambiguities, rather than just picking the best state at each individual moment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}